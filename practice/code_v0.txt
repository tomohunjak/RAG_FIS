import os
import re
import csv
import json
import uuid
import argparse
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple

from pypdf import PdfReader
import chromadb
from chromadb.config import Settings

# Ollama Python client
import ollama


# -------------------------
# Config
# -------------------------

@dataclass
class RAGConfig:
    persist_dir: str = "rag_store"
    collection_name: str = "slr_rag"

    # Ollama models
    embed_model: str = "nomic-embed-text"
    gen_model: str = "llama3"

    # Chunking for full texts
    chunk_chars: int = 750
    chunk_overlap: int = 150

    # Retrieval
    top_k: int = 5

    # Traceability
    quote_max_chars: int = 400


# -------------------------
# Utilities
# -------------------------

def clean_text(text: str) -> str:
    """Light cleaning; keep it conservative to avoid damaging meaning."""
    if not text:
        return ""
    text = text.replace("\x00", " ")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def stable_doc_id(path_or_key: str) -> str:
    """Deterministic-ish ID: keep filenames stable; fall back to UUID if needed."""
    base = os.path.basename(path_or_key)
    base = re.sub(r"\W+", "_", base).strip("_").lower()
    return base if base else str(uuid.uuid4())

def embed_text(cfg: RAGConfig, text: str) -> List[float]:
    """Ollama embeddings API."""
    resp = ollama.embeddings(model=cfg.embed_model, prompt=text)
    return resp["embedding"]

def extract_pdf_text_by_page(pdf_path: str) -> List[str]:
    """Return list of page texts (1-based pages in metadata later)."""
    reader = PdfReader(pdf_path)
    pages = []
    for p in reader.pages:
        t = p.extract_text() or ""
        pages.append(clean_text(t))
    return pages

def chunk_fulltext(pages: List[str], cfg: RAGConfig) -> List[Dict]:
    """
    Boundary-respecting chunking:
    - Preserve document boundaries (no cross-document chunks).
    - Full text: chunk sequentially across pages using char windows.
    - Keep page provenance for traceability.
    """
    # Concatenate with explicit page separators while retaining page mapping
    # We'll build a big string and also a char->page mapping.
    joined = []
    char_page = []  # for each char position, which page (1-based) it came from
    for i, page_text in enumerate(pages, start=1):
        if not page_text:
            continue
        # Add page separator
        sep = f"\n\n[PAGE {i}]\n\n"
        joined.append(sep + page_text)
    full = "".join(joined)
    full = clean_text(full)

    # Build chunks
    chunks = []
    n = len(full)
    start = 0
    while start < n:
        end = min(start + cfg.chunk_chars, n)
        chunk = full[start:end]

        # Try to end on a boundary (sentence-ish) to reduce mid-sentence splits
        if end < n:
            boundary = max(chunk.rfind(". "), chunk.rfind("\n\n"), chunk.rfind("; "))
            if boundary > int(cfg.chunk_chars * 0.6):
                end = start + boundary + 1
                chunk = full[start:end]

        chunk = clean_text(chunk)
        if chunk:
            # Approximate page range by parsing [PAGE X] markers inside chunk
            pages_in_chunk = re.findall(r"\[PAGE (\d+)\]", chunk)
            if pages_in_chunk:
                pmin = int(pages_in_chunk[0])
                pmax = int(pages_in_chunk[-1])
            else:
                pmin = None
                pmax = None

            chunks.append({
                "text": chunk,
                "page_start": pmin,
                "page_end": pmax
            })

        # Overlap
        if end == n:
            break
        start = max(0, end - cfg.chunk_overlap)

    return chunks

def load_abstracts_csv(csv_path: str) -> List[Dict]:
    """
    Expected columns (flexible):
      - id (optional)
      - title (optional)
      - year (optional)
      - authors (optional)
      - abstract (required-ish)
      - doi (optional)
    Each row is treated as its own document (SLR rule).
    """
    rows = []
    with open(csv_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for r in reader:
            abstract = (r.get("abstract") or r.get("Abstract") or "").strip()
            if not abstract:
                continue
            rows.append({
                "id": (r.get("id") or r.get("ID") or "").strip(),
                "title": (r.get("title") or r.get("Title") or "").strip(),
                "year": (r.get("year") or r.get("Year") or "").strip(),
                "authors": (r.get("authors") or r.get("Authors") or "").strip(),
                "doi": (r.get("doi") or r.get("DOI") or "").strip(),
                "abstract": clean_text(abstract)
            })
    return rows


# -------------------------
# Vector store (Chroma)
# -------------------------

def get_chroma_client(cfg: RAGConfig):
    os.makedirs(cfg.persist_dir, exist_ok=True)
    return chromadb.PersistentClient(
        path=cfg.persist_dir,
        settings=Settings(anonymized_telemetry=False)
    )

def get_collection(cfg: RAGConfig):
    client = get_chroma_client(cfg)
    return client.get_or_create_collection(name=cfg.collection_name, metadata={"hnsw:space": "cosine"})


# -------------------------
# Ingestion
# -------------------------

def ingest_pdfs(cfg: RAGConfig, pdf_dir: str):
    col = get_collection(cfg)
    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith(".pdf")]
    if not pdf_files:
        print(f"No PDFs found in: {pdf_dir}")
        return

    for fname in sorted(pdf_files):
        path = os.path.join(pdf_dir, fname)
        doc_id = stable_doc_id(fname)
        print(f"Ingesting PDF: {fname} -> doc_id={doc_id}")

        pages = extract_pdf_text_by_page(path)
        # If extraction fails heavily, you’ll see empty pages; this pipeline assumes text PDFs.
        chunks = chunk_fulltext(pages, cfg)

        # Add chunks to vector store
        ids = []
        docs = []
        metas = []
        embs = []

        for i, ch in enumerate(chunks):
            chunk_id = f"{doc_id}__full__{i}"
            text = ch["text"]
            emb = embed_text(cfg, text)

            ids.append(chunk_id)
            docs.append(text)
            metas.append({
                "doc_id": doc_id,
                "source_type": "fulltext",
                "filename": fname,
                "title": fname,              # you can replace with parsed metadata later
                "page_start": ch["page_start"],
                "page_end": ch["page_end"],
                "chunk_index": i
            })
            embs.append(emb)

        if ids:
            col.add(ids=ids, documents=docs, metadatas=metas, embeddings=embs)
            print(f"  Added {len(ids)} chunks.")
        else:
            print("  No text chunks extracted (PDF may be scanned or empty).")

def ingest_abstracts(cfg: RAGConfig, abstracts_csv: str):
    col = get_collection(cfg)
    rows = load_abstracts_csv(abstracts_csv)
    if not rows:
        print(f"No abstracts found in: {abstracts_csv}")
        return

    print(f"Ingesting abstracts from CSV: {abstracts_csv} ({len(rows)} rows)")

    ids = []
    docs = []
    metas = []
    embs = []

    for idx, r in enumerate(rows):
        # Each abstract is its own document (your rule)
        base = r["id"] if r["id"] else f"abstract_{idx}"
        doc_id = stable_doc_id(base)

        text = r["abstract"]
        emb = embed_text(cfg, text)

        chunk_id = f"{doc_id}__abstract__0"

        ids.append(chunk_id)
        docs.append(text)
        metas.append({
            "doc_id": doc_id,
            "source_type": "abstract",
            "filename": os.path.basename(abstracts_csv),
            "title": r["title"] or doc_id,
            "year": r["year"],
            "authors": r["authors"],
            "doi": r["doi"],
            "chunk_index": 0
        })
        embs.append(emb)

    col.add(ids=ids, documents=docs, metadatas=metas, embeddings=embs)
    print(f"  Added {len(ids)} abstracts (each as a single chunk).")


# -------------------------
# Retrieval + Traceable answering
# -------------------------

def retrieve(cfg: RAGConfig, query: str, top_k: Optional[int] = None) -> List[Tuple[str, Dict, float]]:
    col = get_collection(cfg)
    k = top_k or cfg.top_k

    q_emb = embed_text(cfg, query)
    res = col.query(query_embeddings=[q_emb], n_results=k, include=["documents", "metadatas", "distances", "ids"])

    out = []
    for doc, meta, dist, cid in zip(res["documents"][0], res["metadatas"][0], res["distances"][0], res["ids"][0]):
        # Chroma returns distance; for cosine it’s (1 - cosine_similarity) usually.
        out.append((cid, meta, float(dist), doc))
    return out

def build_traceable_context(cfg: RAGConfig, retrieved: List[Tuple[str, Dict, float]], query: str) -> str:
    """
    Provide excerpts with explicit source tags to enable traceability.
    """
    blocks = []
    for cid, meta, dist, doc in retrieved:
        # Keep excerpt short for prompt budget
        excerpt = doc[:cfg.quote_max_chars]
        title = meta.get("title") or meta.get("filename") or meta.get("doc_id")
        src_type = meta.get("source_type", "unknown")
        pages = ""
        if meta.get("page_start") is not None and meta.get("page_end") is not None:
            pages = f", pages {meta['page_start']}-{meta['page_end']}"
        blocks.append(
            f"[SOURCE {cid}] ({src_type}; {title}{pages})\n"
            f"EXCERPT:\n{excerpt}\n"
        )

    context = "\n---\n".join(blocks)

    prompt = f"""You are assisting with a literature-based research task.
You MUST use only the provided SOURCES. If the sources do not support a claim, say you cannot determine it from the corpus.

Task: Answer the question and make the answer TRACEABLE:
- After each substantive claim, include a source tag in square brackets, e.g. [SOURCE doc__full__3].
- Do NOT invent citations.
- If multiple sources support a claim, you may cite multiple tags.

Question: {query}

SOURCES:
{context}

Write:
1) Answer (with source tags)
2) Sources used (list the SOURCE ids you cited)
"""
    return prompt

def answer_query(cfg: RAGConfig, query: str, top_k: Optional[int] = None) -> Dict:
    retrieved = retrieve(cfg, query, top_k=top_k)
    prompt = build_traceable_context(cfg, retrieved, query)

    resp = ollama.generate(model=cfg.gen_model, prompt=prompt)
    text = resp["response"]

    # Package “evidence” so you can show it under the answer
    evidence = []
    for cid, meta, dist, doc in retrieved:
        evidence.append({
            "source_id": cid,
            "distance": dist,
            "title": meta.get("title"),
            "filename": meta.get("filename"),
            "source_type": meta.get("source_type"),
            "page_start": meta.get("page_start"),
            "page_end": meta.get("page_end"),
            "excerpt": doc[:cfg.quote_max_chars]
        })

    return {"answer": text, "evidence": evidence}


# -------------------------
# CLI
# -------------------------

def main():
    parser = argparse.ArgumentParser(description="Local SLR-grounded RAG pipeline with explicit traceability.")
    sub = parser.add_subparsers(dest="cmd", required=True)

    p_ingest = sub.add_parser("ingest", help="Ingest PDFs and/or abstracts into the vector store.")
    p_ingest.add_argument("--pdf_dir", default="data/pdfs", help="Folder with PDFs.")
    p_ingest.add_argument("--abstracts_csv", default="", help="Optional CSV with abstracts (one per row).")

    p_ask = sub.add_parser("ask", help="Ask a question and get an answer with sources.")
    p_ask.add_argument("query", help="Your question")
    p_ask.add_argument("--top_k", type=int, default=5, help="Number of retrieved chunks")

    args = parser.parse_args()
    cfg = RAGConfig()

    if args.cmd == "ingest":
        if os.path.isdir(args.pdf_dir):
            ingest_pdfs(cfg, args.pdf_dir)
        else:
            print(f"PDF dir not found: {args.pdf_dir}")

        if args.abstracts_csv:
            if os.path.isfile(args.abstracts_csv):
                ingest_abstracts(cfg, args.abstracts_csv)
            else:
                print(f"Abstracts CSV not found: {args.abstracts_csv}")

        print("\nDone. Vector store persisted in:", cfg.persist_dir)

    elif args.cmd == "ask":
        result = answer_query(cfg, args.query, top_k=args.top_k)

        print("\n=== ANSWER (traceable) ===\n")
        print(result["answer"].strip())

        print("\n=== RETRIEVED EVIDENCE (for manual checking) ===\n")
        for e in result["evidence"]:
            pages = ""
            if e["page_start"] is not None and e["page_end"] is not None:
                pages = f" pages {e['page_start']}-{e['page_end']}"
            print(f"- {e['source_id']} | {e['source_type']} | {e.get('title')}{pages}")
            print(f"  excerpt: {e['excerpt'][:200].replace('\\n', ' ')}...")
            print()

if __name__ == "__main__":
    main()
